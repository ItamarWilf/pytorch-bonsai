[net]
width=416
height=416
in_channels=4

# layer 1
[prunable_conv2d]
batch_normalize=0
out_channels=32
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 2
[prunable_conv2d]
batch_normalize=0
out_channels=32
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of inconv


# layer 3
[maxpool]
kernel_size=2
stride=2

# layer 4
[prunable_conv2d]
batch_normalize=0
out_channels=64
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 5
[prunable_conv2d]
batch_normalize=0
out_channels=64
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of down 1

# layer 6
[maxpool]
kernel_size=2
stride=2

# layer 7
[prunable_conv2d]
batch_normalize=0
out_channels=128
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 8
[prunable_conv2d]
batch_normalize=0
out_channels=128
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of down 2


# layer 9
[maxpool]
kernel_size=2
stride=2

# layer 10
[prunable_conv2d]
batch_normalize=0
out_channels=256
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 11
[prunable_conv2d]
batch_normalize=0
out_channels=256
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of down 3

# layer 12
[maxpool]
kernel_size=2
stride=2

# layer 13
[prunable_conv2d]
batch_normalize=0
out_channels=512
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 14
[prunable_conv2d]
batch_normalize=0
out_channels=512
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of down 4

# layer 15
[prunable_deconv2d]
batch_normalize=0
out_channels=256
kernel_size=2
stride=2
padding=0

# layer 16
[route]
layers=-5,-1
# 15 and 11

# layer 17
[prunable_conv2d]
batch_normalize=0
out_channels=256
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 18
[prunable_conv2d]
batch_normalize=0
out_channels=256
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of up1

# layer 19
[prunable_deconv2d]
batch_normalize=0
out_channels=128
kernel_size=2
stride=2
padding=0

# layer 20
[route]
layers=-12,-1
# 19 and 8

# layer 21
[prunable_conv2d]
batch_normalize=0
out_channels=128
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 22
[prunable_conv2d]
batch_normalize=0
out_channels=128
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of up2

# layer 23
[prunable_deconv2d]
batch_normalize=0
out_channels=64
kernel_size=2
stride=2
padding=0

# layer 24
[route]
layers=-19,-1
# 23 and 5

# layer 25
[prunable_conv2d]
batch_normalize=0
out_channels=64
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 26
[prunable_conv2d]
batch_normalize=0
out_channels=64
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of up3

# layer 27
[prunable_deconv2d]
batch_normalize=0
out_channels=32
kernel_size=2
stride=2
padding=0

# layer 28
[route]
layers=-26,-1
# 27 and 2

# layer 29
[prunable_conv2d]
batch_normalize=0
out_channels=32
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2

# layer 30
[prunable_conv2d]
batch_normalize=0
out_channels=32
kernel_size=3
stride=1
padding=1
activation=leaky_relu
negative_slope=0.2
# end of up4

# out conv
# layer 31
[conv2d]
batch_normalize=0
out_channels=12
kernel_size=1
stride=1
padding=0
negative_slope=0.2


[pixel_shuffle]
upscale_factor=2
output=1