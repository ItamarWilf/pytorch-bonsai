[net]
height=32
width=32
in_channels=3

#0
[prunable_conv2d]
batch_normalize=1
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False
activation=ReLU

#1
[prunable_conv2d]
batch_normalize=1
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False
activation=ReLU

#2
[prunable_conv2d]
batch_normalize=1
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#3
[residual_add]
layers=-1,-3
activation=ReLU

#4
[prunable_conv2d]
batch_normalize=1
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False
activation=ReLU

#5
[prunable_conv2d]
batch_normalize=1
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#6
[residual_add]
layers=-1,-3
activation=ReLU

#7
[prunable_conv2d]
batch_normalize=1
out_channels=128
kernel_size=3
stride=2
padding=1
bias=False
activation=ReLU

#8
[prunable_conv2d]
batch_normalize=1
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

#9
[route]
layers=-3

#10
[prunable_conv2d]
batch_normalize=1
out_channels=128
kernel_size=1
stride=2
bias=False

#11
[residual_add]
layers=-1,-3
activation=ReLU

#12
[prunable_conv2d]
batch_normalize=1
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False
activation=ReLU

#13
[prunable_conv2d]
batch_normalize=1
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

#14
[residual_add]
layers=-1,-3
activation=ReLU

#15
[prunable_conv2d]
batch_normalize=1
out_channels=256
kernel_size=3
stride=2
padding=1
bias=False
activation=ReLU

#16
[prunable_conv2d]
batch_normalize=1
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

#17
[route]
layers=-3

#18
[prunable_conv2d]
batch_normalize=1
out_channels=256
kernel_size=1
stride=2
bias=False

#19
[residual_add]
layers=-1,-3
activation=ReLU

#20
[prunable_conv2d]
batch_normalize=1
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False
activation=ReLU

#21
[prunable_conv2d]
batch_normalize=1
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

#22
[residual_add]
layers=-1,-3
activation=ReLU

#23
[prunable_conv2d]
batch_normalize=1
out_channels=512
kernel_size=3
stride=2
padding=1
bias=False
activation=ReLU

#24
[prunable_conv2d]
batch_normalize=1
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

#25
[route]
layers=-3

#26
[prunable_conv2d]
batch_normalize=1
out_channels=512
kernel_size=1
stride=2
bias=False

#27
[residual_add]
layers=-1,-3
activation=ReLU

#28
[prunable_conv2d]
batch_normalize=1
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False
activation=ReLU

#29
[prunable_conv2d]
batch_normalize=1
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

#30
[residual_add]
layers=-1,-3
activation=ReLU

#31
[avgpool2d]
kernel_size=4

#32
[flatten]

#
[linear]
out_features=10
output=1