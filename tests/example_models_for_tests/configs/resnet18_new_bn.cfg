[net]
height=32
width=32
in_channels=3

#0
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#1
[batchnorm2d]
activation=LeakyReLU

#2
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#3
[batchnorm2d]
activation=LeakyReLU

#4
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#5
[batchnorm2d]

#6
[residual_add]
layers=-1,-5
activation=LeakyReLU

#7
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#8
[batchnorm2d]
activation=LeakyReLU

#9
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

#10
[batchnorm2d]

#11
[residual_add]
layers=-1,-5
activation=LeakyReLU

#12
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=2
padding=1
bias=False

#13
[batchnorm2d]
activation=LeakyReLU

#14
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

#15
[batchnorm2d]

#16
[route]
layers=-5

#17
[prunable_conv2d]
out_channels=128
kernel_size=1
stride=2
bias=False

#18
[batchnorm2d]

#19
[residual_add]
layers=-1,-4
activation=LeakyReLU

#20
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

#21
[batchnorm2d]
activation=LeakyReLU

#22
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

#23
[batchnorm2d]

#24
[residual_add]
layers=-1,-5
activation=LeakyReLU

#25
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=2
padding=1
bias=False

#26
[batchnorm2d]
activation=LeakyReLU

#27
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

#28
[batchnorm2d]

#29
[route]
layers=-5

#30
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=2
bias=False

#31
[batchnorm2d]

#32
[residual_add]
layers=-1,-4
activation=LeakyReLU

#33
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

#34
[batchnorm2d]
activation=LeakyReLU

#35
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

#36
[batchnorm2d]

#37
[residual_add]
layers=-1,-5
activation=LeakyReLU

#38
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=2
padding=1
bias=False

#39
[batchnorm2d]
activation=LeakyReLU

#40
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

#41
[batchnorm2d]

#42
[route]
layers=-5

#43
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=2
bias=False

#44
[batchnorm2d]

#45
[residual_add]
layers=-1,-4
activation=LeakyReLU

#46
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

#47
[batchnorm2d]
activation=LeakyReLU

#48
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

#49
[batchnorm2d]

#50
[residual_add]
layers=-1,-5
activation=LeakyReLU

#31
[avgpool2d]
kernel_size=4

#32
[flatten]

#
[linear]
out_features=10
output=1