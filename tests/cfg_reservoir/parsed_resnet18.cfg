[net]
width=32
height=32
in_channels=3

# layer 1
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 2
[batchnorm2d]
activation=ReLU

# layer 3
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 4
[batchnorm2d]
activation=ReLU

# layer 5
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 6
[batchnorm2d]

# layer 7
[residual_add]
layers=-1, -5
activation=ReLU

# layer 8
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 9
[batchnorm2d]
activation=ReLU

# layer 10
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 11
[batchnorm2d]

# layer 12
[residual_add]
layers=-1, -5
activation=ReLU

# layer 13
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=2
padding=1
bias=False

# layer 14
[batchnorm2d]
activation=ReLU

# layer 15
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

# layer 16
[batchnorm2d]

# layer 17
[route]
layers=-5

# layer 18
[prunable_conv2d]
out_channels=128
kernel_size=1
stride=2
padding=0
bias=False

# layer 19
[batchnorm2d]

# layer 20
[residual_add]
layers=-4, -1
activation=ReLU

# layer 21
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

# layer 22
[batchnorm2d]
activation=ReLU

# layer 23
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

# layer 24
[batchnorm2d]

# layer 25
[residual_add]
layers=-1, -5
activation=ReLU

# layer 26
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=2
padding=1
bias=False

# layer 27
[batchnorm2d]
activation=ReLU

# layer 28
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 29
[batchnorm2d]

# layer 30
[route]
layers=-5

# layer 31
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=2
padding=0
bias=False

# layer 32
[batchnorm2d]

# layer 33
[residual_add]
layers=-4, -1
activation=ReLU

# layer 34
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 35
[batchnorm2d]
activation=ReLU

# layer 36
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 37
[batchnorm2d]

# layer 38
[residual_add]
layers=-1, -5
activation=ReLU

# layer 39
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=2
padding=1
bias=False

# layer 40
[batchnorm2d]
activation=ReLU

# layer 41
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

# layer 42
[batchnorm2d]

# layer 43
[route]
layers=-5

# layer 44
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=2
padding=0
bias=False

# layer 45
[batchnorm2d]

# layer 46
[residual_add]
layers=-4, -1
activation=ReLU

# layer 47
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

# layer 48
[batchnorm2d]
activation=ReLU

# layer 49
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

# layer 50
[batchnorm2d]

# layer 51
[residual_add]
layers=-1, -5
activation=ReLU

# layer 52
[avgpool2d]
kernel_size=4

# layer 53
[flatten]

# layer 54
[linear]
in_features=512
out_features=10
output=1

