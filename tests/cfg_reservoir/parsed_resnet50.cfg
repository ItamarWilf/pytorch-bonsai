[net]
width=32
height=32
in_channels=3

# layer 1
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 2
[batchnorm2d]
activation=ReLU

# layer 3
[prunable_conv2d]
out_channels=64
kernel_size=1
stride=1
padding=0
bias=False

# layer 4
[batchnorm2d]
activation=ReLU

# layer 5
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 6
[batchnorm2d]
activation=ReLU

# layer 7
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 8
[batchnorm2d]

# layer 9
[route]
layers=-7

# layer 10
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 11
[batchnorm2d]

# layer 12
[residual_add]
layers=-4, -1
activation=ReLU

# layer 13
[prunable_conv2d]
out_channels=64
kernel_size=1
stride=1
padding=0
bias=False

# layer 14
[batchnorm2d]
activation=ReLU

# layer 15
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 16
[batchnorm2d]
activation=ReLU

# layer 17
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 18
[batchnorm2d]

# layer 19
[residual_add]
layers=-1, -7
activation=ReLU

# layer 20
[prunable_conv2d]
out_channels=64
kernel_size=1
stride=1
padding=0
bias=False

# layer 21
[batchnorm2d]
activation=ReLU

# layer 22
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=False

# layer 23
[batchnorm2d]
activation=ReLU

# layer 24
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 25
[batchnorm2d]

# layer 26
[residual_add]
layers=-1, -7
activation=ReLU

# layer 27
[prunable_conv2d]
out_channels=128
kernel_size=1
stride=1
padding=0
bias=False

# layer 28
[batchnorm2d]
activation=ReLU

# layer 29
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=2
padding=1
bias=False

# layer 30
[batchnorm2d]
activation=ReLU

# layer 31
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 32
[batchnorm2d]

# layer 33
[route]
layers=-7

# layer 34
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=2
padding=0
bias=False

# layer 35
[batchnorm2d]

# layer 36
[residual_add]
layers=-4, -1
activation=ReLU

# layer 37
[prunable_conv2d]
out_channels=128
kernel_size=1
stride=1
padding=0
bias=False

# layer 38
[batchnorm2d]
activation=ReLU

# layer 39
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

# layer 40
[batchnorm2d]
activation=ReLU

# layer 41
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 42
[batchnorm2d]

# layer 43
[residual_add]
layers=-1, -7
activation=ReLU

# layer 44
[prunable_conv2d]
out_channels=128
kernel_size=1
stride=1
padding=0
bias=False

# layer 45
[batchnorm2d]
activation=ReLU

# layer 46
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

# layer 47
[batchnorm2d]
activation=ReLU

# layer 48
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 49
[batchnorm2d]

# layer 50
[residual_add]
layers=-1, -7
activation=ReLU

# layer 51
[prunable_conv2d]
out_channels=128
kernel_size=1
stride=1
padding=0
bias=False

# layer 52
[batchnorm2d]
activation=ReLU

# layer 53
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=False

# layer 54
[batchnorm2d]
activation=ReLU

# layer 55
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 56
[batchnorm2d]

# layer 57
[residual_add]
layers=-1, -7
activation=ReLU

# layer 58
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 59
[batchnorm2d]
activation=ReLU

# layer 60
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=2
padding=1
bias=False

# layer 61
[batchnorm2d]
activation=ReLU

# layer 62
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=1
padding=0
bias=False

# layer 63
[batchnorm2d]

# layer 64
[route]
layers=-7

# layer 65
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=2
padding=0
bias=False

# layer 66
[batchnorm2d]

# layer 67
[residual_add]
layers=-4, -1
activation=ReLU

# layer 68
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 69
[batchnorm2d]
activation=ReLU

# layer 70
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 71
[batchnorm2d]
activation=ReLU

# layer 72
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=1
padding=0
bias=False

# layer 73
[batchnorm2d]

# layer 74
[residual_add]
layers=-1, -7
activation=ReLU

# layer 75
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 76
[batchnorm2d]
activation=ReLU

# layer 77
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 78
[batchnorm2d]
activation=ReLU

# layer 79
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=1
padding=0
bias=False

# layer 80
[batchnorm2d]

# layer 81
[residual_add]
layers=-1, -7
activation=ReLU

# layer 82
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 83
[batchnorm2d]
activation=ReLU

# layer 84
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 85
[batchnorm2d]
activation=ReLU

# layer 86
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=1
padding=0
bias=False

# layer 87
[batchnorm2d]

# layer 88
[residual_add]
layers=-1, -7
activation=ReLU

# layer 89
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 90
[batchnorm2d]
activation=ReLU

# layer 91
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 92
[batchnorm2d]
activation=ReLU

# layer 93
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=1
padding=0
bias=False

# layer 94
[batchnorm2d]

# layer 95
[residual_add]
layers=-1, -7
activation=ReLU

# layer 96
[prunable_conv2d]
out_channels=256
kernel_size=1
stride=1
padding=0
bias=False

# layer 97
[batchnorm2d]
activation=ReLU

# layer 98
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=False

# layer 99
[batchnorm2d]
activation=ReLU

# layer 100
[prunable_conv2d]
out_channels=1024
kernel_size=1
stride=1
padding=0
bias=False

# layer 101
[batchnorm2d]

# layer 102
[residual_add]
layers=-1, -7
activation=ReLU

# layer 103
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 104
[batchnorm2d]
activation=ReLU

# layer 105
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=2
padding=1
bias=False

# layer 106
[batchnorm2d]
activation=ReLU

# layer 107
[prunable_conv2d]
out_channels=2048
kernel_size=1
stride=1
padding=0
bias=False

# layer 108
[batchnorm2d]

# layer 109
[route]
layers=-7

# layer 110
[prunable_conv2d]
out_channels=2048
kernel_size=1
stride=2
padding=0
bias=False

# layer 111
[batchnorm2d]

# layer 112
[residual_add]
layers=-4, -1
activation=ReLU

# layer 113
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 114
[batchnorm2d]
activation=ReLU

# layer 115
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

# layer 116
[batchnorm2d]
activation=ReLU

# layer 117
[prunable_conv2d]
out_channels=2048
kernel_size=1
stride=1
padding=0
bias=False

# layer 118
[batchnorm2d]

# layer 119
[residual_add]
layers=-1, -7
activation=ReLU

# layer 120
[prunable_conv2d]
out_channels=512
kernel_size=1
stride=1
padding=0
bias=False

# layer 121
[batchnorm2d]
activation=ReLU

# layer 122
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=False

# layer 123
[batchnorm2d]
activation=ReLU

# layer 124
[prunable_conv2d]
out_channels=2048
kernel_size=1
stride=1
padding=0
bias=False

# layer 125
[batchnorm2d]

# layer 126
[residual_add]
layers=-1, -7
activation=ReLU

# layer 127
[avgpool2d]
kernel_size=4

# layer 128
[flatten]

# layer 129
[linear]
in_features=2048
out_features=10
output=1

