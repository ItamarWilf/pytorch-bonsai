[net]
width=32
height=32
in_channels=3

# layer 1
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=True

# layer 2
[batchnorm2d]
activation=ReLU

# layer 3
[prunable_conv2d]
out_channels=64
kernel_size=3
stride=1
padding=1
bias=True

# layer 4
[batchnorm2d]
activation=ReLU

# layer 5
[maxpool]
kernel_size=2
stride=2

# layer 6
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=True

# layer 7
[batchnorm2d]
activation=ReLU

# layer 8
[prunable_conv2d]
out_channels=128
kernel_size=3
stride=1
padding=1
bias=True

# layer 9
[batchnorm2d]
activation=ReLU

# layer 10
[maxpool]
kernel_size=2
stride=2

# layer 11
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=True

# layer 12
[batchnorm2d]
activation=ReLU

# layer 13
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=True

# layer 14
[batchnorm2d]
activation=ReLU

# layer 15
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=True

# layer 16
[batchnorm2d]
activation=ReLU

# layer 17
[prunable_conv2d]
out_channels=256
kernel_size=3
stride=1
padding=1
bias=True

# layer 18
[batchnorm2d]
activation=ReLU

# layer 19
[maxpool]
kernel_size=2
stride=2

# layer 20
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 21
[batchnorm2d]
activation=ReLU

# layer 22
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 23
[batchnorm2d]
activation=ReLU

# layer 24
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 25
[batchnorm2d]
activation=ReLU

# layer 26
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 27
[batchnorm2d]
activation=ReLU

# layer 28
[maxpool]
kernel_size=2
stride=2

# layer 29
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 30
[batchnorm2d]
activation=ReLU

# layer 31
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 32
[batchnorm2d]
activation=ReLU

# layer 33
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 34
[batchnorm2d]
activation=ReLU

# layer 35
[prunable_conv2d]
out_channels=512
kernel_size=3
stride=1
padding=1
bias=True

# layer 36
[batchnorm2d]
activation=ReLU

# layer 37
[maxpool]
kernel_size=2
stride=2

# layer 38
[avgpool2d]
kernel_size=1

# layer 39
[flatten]

# layer 40
[linear]
in_features=512
out_features=10
output=1

